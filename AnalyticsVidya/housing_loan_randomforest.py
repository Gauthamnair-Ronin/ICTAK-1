# -*- coding: utf-8 -*-
"""Housing_loan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TSknABUbh8pXcNDhvZyx3fc4-dFYXKyf
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.preprocessing import LabelEncoder,OrdinalEncoder

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix

# importing data

df=pd.read_csv("train.csv")
df2=pd.read_csv("test.csv")
df

df2

df.describe()

df2.describe()

df.info()

df.shape

# lets check for abnormal values by taking unique values present in categorical data
print(df['Gender'].unique())
print(df['Married'].unique())
print(df['Dependents'].unique())
print(df['Education'].unique())
print(df['Self_Employed'].unique())
print(df['Property_Area'].unique())
print(df['Loan_Status'].unique())
print(df['Credit_History'].unique())

# lets check for abnormal values in test by taking unique values present in categorical data
print(df2['Gender'].unique())
print(df2['Married'].unique())
print(df2['Dependents'].unique())
print(df2['Education'].unique())
print(df2['Self_Employed'].unique())
print(df2['Property_Area'].unique())
print(df2['Credit_History'].unique())

#changing dependents in test data numeric
df2['Dependents']=df2['Dependents'].replace('3+','3')
df2['Dependents']=pd.to_numeric(df2['Dependents'])
print(df2['Dependents'].unique())

# there seems to be nan values in gender, married, dependents, self_employed and loan credit.
# HANDLING MISSING VALUES

# gender
df['Gender']=df["Gender"].fillna(df['Gender'].mode()[0])
df['Gender'].isna().sum()

#married

#df['Married'].isna().sum()
#df['Married'].value_counts() since count of yes is more than no, we will replace missig values with yes

df["Married"]= df["Married"].fillna(df["Married"].mode()[0])
df["Married"].isna().sum()

# Dependents

#df["Dependents"].info() since type is object, we have to chance it to numeric. but before that we need to replace the object 3+ with 3
df['Dependents']=df['Dependents'].replace('3+','3')
print(df['Dependents'].unique())
print(df['Dependents'].value_counts())

df['Dependents'].isna().sum()

print(df['Dependents'][df["Married"]=='Yes'].value_counts())
df['Dependents'][df["Married"]=='No'].value_counts()

# married or not, dependents are mostly 0. therefor filling nan with 0
df['Dependents']=df["Dependents"].fillna('0')
df["Dependents"]=pd.to_numeric(df["Dependents"])
df["Dependents"].info()

# self employment
print("Nan count = ",df['Self_Employed'].isna().sum())
print(df['Self_Employed'].value_counts())
# since No outweighs yes significantly, lets replace the 32 nan values with NO


df['Self_Employed']=df['Self_Employed'].fillna('No')
print("After fixing nan values")
print("Nan count = ",df['Self_Employed'].isna().sum())
print(df['Self_Employed'].value_counts())

# Credit history
print(df['Credit_History'].isna().sum())
print(df['Credit_History'].value_counts())


# fillling nans with 1

df['Credit_History']=df['Credit_History'].fillna(1)
print(df['Credit_History'].isna().sum())
print(df['Credit_History'].value_counts())

"""All missing values in categorical data has been handled

"""

df['Loan_Status'].value_counts()

# we have to upsample the No value data to avoid disparity

yes_data = df[df['Loan_Status'] == 'Y']
no_data = df[df['Loan_Status'] == 'N']

no_data_upsampled = no_data.sample(n=len(yes_data), replace=True, random_state=42)

df = pd.concat([yes_data, no_data_upsampled])
df['Loan_Status'].value_counts()

#df

num_df=df.select_dtypes(include=['int64','float64'])
#num_df

# since dependents and credit_history is encoded categorical data, lets drop it and add it to categorical data column
cat_df= df.select_dtypes(include=['object'])
cat_df=pd.concat([cat_df,num_df[['Dependents','Credit_History']]],axis=1)
num_df.drop(['Dependents','Credit_History'],axis=1,inplace=True)
#cat_df

# since loan_status is target, we need to isolate it. but before that lets check for missing loan status values
print(cat_df["Loan_Status"].isna().sum())

target=cat_df['Loan_Status']
cat_df.drop("Loan_Status",axis=1,inplace=True)

# handling missing values in numeical data
print(num_df.isna().sum())
num_df

print(num_df['CoapplicantIncome'].isna().sum(),"\n\n")
#print(num_df['CoapplicantIncome'].unique())

# no missing values in coapplicantincome

print(num_df['ApplicantIncome'].isna().sum(),"\n\n")
#print(num_df['ApplicantIncome'].unique())

print(num_df['Loan_Amount_Term'].isna().sum(),"\n\n")
print(num_df['Loan_Amount_Term'].unique())

# it seems loan amount term too is categorical data

cat_df=pd.concat([cat_df,num_df["Loan_Amount_Term"]],axis=1)
num_df.drop("Loan_Amount_Term",axis=1,inplace=True)
cat_df

# filling missing values of loan amount term
print("nan count =",cat_df['Loan_Amount_Term'].isna().sum(),"\n")
print(cat_df["Loan_Amount_Term"].value_counts())

# since count of value 360 outweighs all other significantly, nan can be filled with 360
cat_df['Loan_Amount_Term']=cat_df['Loan_Amount_Term'].fillna(360)
print("nan count =",cat_df['Loan_Amount_Term'].isna().sum(),"\n")

# loan amount
print("nan count =",num_df["LoanAmount"].isna().sum,"\n\n")

num_df["LoanAmount"].describe()

# based on describe the maximum value seems to be a bit too deviated from the trend of quantiles.
# Max value could potentially be an outlier
plt.scatter(num_df["LoanAmount"],num_df.index)
plt.xlabel("LoanAmount")
plt.show()

# since graph too is indicating potential outliers, lets handle outliers before filling nan values.
# but i wouldnt be removing the outliers, handling the ouliers only to get true mean value to fill nan values
num_df_dummy=num_df.copy()
def replace_outliers(df,col_name):
    q1= df[col_name].quantile(0.25)
    q3=df[col_name].quantile(0.75)
    iqr=q3-q1
    up_bound= q3 + 1.5*iqr
    low_bound= q1 - 1.5* iqr

    #outliers = df[(df[col_name] < low_bound) | (df[col_name] > up_bound)]
    df[col_name]=df[col_name].clip(upper=up_bound)
    df[col_name]=df[col_name].clip(lower=low_bound)
    #print(f"\n{col_name}\n",outliers[col_name])
    return df[col_name]

for col in num_df_dummy.columns:
    num_df_dummy[col]=replace_outliers(num_df_dummy,col)

print("After handling outliers")
plt.boxplot(num_df_dummy)
plt.xlabel("LoanAmount")
plt.show()

m=num_df_dummy['LoanAmount'].mean()
num_df['LoanAmount']=num_df['LoanAmount'].fillna(m)
num_df['LoanAmount'].describe()

num_df.isna().sum()

"""ALL missing values in all columns have been handled

# Scaling the training data
"""

sns.histplot(num_df['ApplicantIncome'])
plt.xlabel("ApplicantIncome")
plt.show()
sns.histplot(num_df['CoapplicantIncome'])
plt.xlabel("CoapplicantIncome")
plt.show()
sns.histplot(num_df['LoanAmount'])
plt.xlabel("LoanAmount")
plt.show()
plt.tight_layout()

# Without handling outliers, all of them seem skewed, so lets use minmax scaling
minmax= MinMaxScaler()
#std= StandardScaler()

num_df['ApplicantIncome']=minmax.fit_transform(num_df[['ApplicantIncome']])
num_df['CoapplicantIncome']=minmax.fit_transform(num_df[['CoapplicantIncome']])
num_df['LoanAmount']=minmax.fit_transform(num_df[['LoanAmount']])

num_df

"""# handling missing values and Scaling test data


"""

df2.isna().sum()



# gender
df2['Gender']=df2["Gender"].fillna(df2['Gender'].mode()[0])
df2['Gender'].isna().sum()
#married

#df['Married'].isna().sum()
#df['Married'].value_counts() since count of yes is more than no, we will replace missig values with yes

df2["Married"]= df2["Married"].fillna(df2["Married"].mode()[0])
df2["Married"].isna().sum()
# Dependents

#df["Dependents"].info() since type is object, we have to chance it to numeric. but before that we need to replace the object 3+ with 3
df2['Dependents']=df2['Dependents'].replace('3+','3')
df2['Dependents'][df2["Married"]=='No'].value_counts()
# married or not, dependents are mostly 0. therefor filling nan with 0
df2['Dependents']=df2["Dependents"].fillna('0')
df2["Dependents"]=pd.to_numeric(df2["Dependents"])

df2['Self_Employed']=df2['Self_Employed'].fillna('No')

# fillling nans with 1

df2['Credit_History']=df2['Credit_History'].fillna(1)

df2.isna().sum()

#df2=df2.fillna(df2['LoanAmount'].mean())
df2['Loan_Amount_Term'].fillna(df2['Loan_Amount_Term'].mode())
df2.isna().sum()

#df2 = df2.dropna(how='any',axis=0)

# num_df2=df2.select_dtypes(include=['int64','float64'])
# cat_df2=df2.select_dtypes(include=['object'])
loan_id=df2['Loan_ID']
# cat_df2=pd.concat([cat_df2,num_df2[['Dependents','Credit_History',"Loan_Amount_Term"]]],axis=1)
# num_df2.drop(['Dependents','Credit_History',"Loan_Amount_Term"],axis=1,inplace=True)
#cat_df2

df2['Loan_Amount_Term'].unique()

minmax= MinMaxScaler()
std= StandardScaler()

df2['ApplicantIncome']=std.fit_transform(df2[['ApplicantIncome']])
df2['CoapplicantIncome']=minmax.fit_transform(df2[['CoapplicantIncome']])
df2['LoanAmount']=std.fit_transform(df2[['LoanAmount']])

df2

"""# Encoding categorical in train data"""

# since loan ID is unique and irrelevant for training, lets drop it
cat_df=cat_df.drop("Loan_ID",axis=1)
cat_df

# Gender, Married, Education, Self_employed , Credit_ history is binary. so lets encode accordingly
lb_enc=LabelEncoder()
cat_df['Gender']=lb_enc.fit_transform(cat_df['Gender'])
cat_df['Married']=lb_enc.fit_transform(cat_df['Married'])
cat_df['Education']=lb_enc.fit_transform(cat_df['Education'])
cat_df['Self_Employed']=lb_enc.fit_transform(cat_df['Self_Employed'])
cat_df['Credit_History']=lb_enc.fit_transform(cat_df['Credit_History'])
cat_df

# Loan_Amount_Term is heirarchical
l=cat_df['Loan_Amount_Term'].unique()
lst=list(np.sort(l))

ord_enc=OrdinalEncoder(categories=[lst])
cat_df['Loan_Amount_Term']=ord_enc.fit_transform(cat_df[['Loan_Amount_Term']])
cat_df

# Property_Area has no order of preference. therefore lets one hot encode it
cat_df=pd.get_dummies(cat_df,columns=['Property_Area'],prefix='col',dtype=int, drop_first=True)
cat_df

"""# Encoding test categories"""

print(df2['Loan_Amount_Term'].unique())
print(final_df['Loan_Amount_Term'].unique())

# since loan ID is unique and irrelevant for training, lets drop it
df2=df2.drop("Loan_ID",axis=1)
# Gender, Married, Education, Self_employed , Credit_ history is binary. so lets encode accordingly
lb_enc=LabelEncoder()
df2['Gender']=lb_enc.fit_transform(df2['Gender'])
df2['Married']=lb_enc.fit_transform(df2['Married'])
df2['Education']=lb_enc.fit_transform(df2['Education'])
df2['Self_Employed']=lb_enc.fit_transform(df2['Self_Employed'])
df2['Credit_History']=lb_enc.fit_transform(df2['Credit_History'])

# Loan_Amount_Term is heirarchical
l=df2['Loan_Amount_Term'].unique()
lst=list(np.sort(l))

ord_enc=OrdinalEncoder(categories=[lst])
df2['Loan_Amount_Term']=ord_enc.fit_transform(df2[['Loan_Amount_Term']])

# Property_Area has no order of preference. therefore lets one hot encode it
df2=pd.get_dummies(df2,columns=['Property_Area'],prefix='col',dtype=int, drop_first=True)
df2

loan_id

"""# Model Training"""

final_df= pd.concat([cat_df,num_df],axis=1)
final_df

X_train,X_test,y_train,y_test=train_test_split(final_df,target,test_size=0.2,random_state=42)
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
y_pred_Knn = knn.predict(X_test)

conf_mat_knn=confusion_matrix(y_test,y_pred_Knn)

plt.figure(figsize=(6, 6))
sns.heatmap(conf_mat_knn, annot=True, fmt="d", cmap="inferno", xticklabels=["No", "Yes"], yticklabels=["No", "Yes"])
plt.title("Confusion Matrix for KNN  Model")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

print("\n ACCURACY KNN : ",accuracy_score(y_test,y_pred_Knn)*100)



"""# Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

conf_mat_rf=confusion_matrix(y_test,y_pred_rf)

plt.figure(figsize=(6, 6))
sns.heatmap(conf_mat_rf, annot=True, fmt="d", cmap="inferno", xticklabels=["No", "Yes"], yticklabels=["No", "Yes"])
plt.title("Confusion Matrix for RandomForest  Model")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

print("\n ACCURACY KNN : ",accuracy_score(y_test,y_pred_rf)*100)

# testing model on test.csv file
final_df2= df2[["Gender","Married","Education","Self_Employed","Dependents","Credit_History","Loan_Amount_Term","col_Semiurban","col_Urban","ApplicantIncome","CoapplicantIncome","LoanAmount"]]

final_df2['Dependents']=final_df2['Dependents'].replace('3+','3')
final_df2['Dependents']=pd.to_numeric(final_df2['Dependents'])

final_df2=final_df2.fillna(0)

test_y_pred_rf=rf.predict(final_df2)
test_y_pred_rf.shape

loan_status=pd.DataFrame(test_y_pred_rf,columns=['Loan_Status'])
# dic= {
#       "LOAN_ID" : loan_id.values,
#       "LOAN_STATUS" : loan_status.values
# }

# dic
loanID=pd.DataFrame(loan_id,columns=['Loan_ID'])
loanID=loanID.reset_index(drop=True)
sub_rf=pd.concat([loanID,loan_status],axis=1)
sub_rf

sub_rf.to_csv("RandomForest_result.csv",index=False)



